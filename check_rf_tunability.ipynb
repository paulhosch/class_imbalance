{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "from skopt.plots import plot_convergence, plot_objective\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "from helpers import get_dataset_path, initialize_model, evaluate_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TUNABLE_PARAMS = {\n",
    "    \"n_estimators\": Integer(100, 1000),\n",
    "    \"max_depth\": Integer(10, 100),\n",
    "    \"min_samples_split\": Integer(2, 20),\n",
    "    \"min_samples_leaf\": Integer(1, 10),\n",
    "    \"max_features\": Categorical([0.3, 0.5, 0.7, 0.9, \"sqrt\", \"log2\"]),  \n",
    "    \"criterion\": Categorical([\"gini\", \"entropy\"])\n",
    "}\n",
    "\n",
    "configurations = {\n",
    "    \"Simple Random Sampling\": {\n",
    "        \"dataset_template\": \"simple_random_{sample_size}.csv\",\n",
    "        \"model\": RandomForestClassifier,\n",
    "        \"fixed_params\": {\n",
    "            \"bootstrap\": True\n",
    "        },\n",
    "        \"tunable_params\": TUNABLE_PARAMS\n",
    "    },\n",
    "    \"SMOTE Balanced Sampling\": {\n",
    "        \"dataset_template\": \"smote_balanced_{sample_size}.csv\",\n",
    "        \"model\": RandomForestClassifier,\n",
    "        \"fixed_params\": {\n",
    "            \"bootstrap\": True\n",
    "        },\n",
    "        \"tunable_params\": TUNABLE_PARAMS\n",
    "    },\n",
    "    \"Stratified Balanced Sampling\": {\n",
    "        \"dataset_template\": \"stratified_balanced_{sample_size}.csv\",\n",
    "        \"model\": RandomForestClassifier,\n",
    "        \"fixed_params\": {\n",
    "            \"bootstrap\": True\n",
    "        },\n",
    "        \"tunable_params\": TUNABLE_PARAMS\n",
    "    },\n",
    "    \"Balanced RF\": {\n",
    "        \"dataset_template\": \"simple_random_{sample_size}.csv\",\n",
    "        \"model\": BalancedRandomForestClassifier,\n",
    "        \"fixed_params\": {\n",
    "            \"sampling_strategy\": \"all\",\n",
    "            \"replacement\": False,\n",
    "            \"bootstrap\": True,\n",
    "            \"oob_score\": False\n",
    "        },\n",
    "        \"tunable_params\": TUNABLE_PARAMS\n",
    "    },\n",
    "    \"Weighted RF\": {\n",
    "        \"dataset_template\": \"simple_random_{sample_size}.csv\",\n",
    "        \"model\": RandomForestClassifier,\n",
    "        \"fixed_params\": {\n",
    "            \"class_weight\": \"balanced\",\n",
    "            \"bootstrap\": True\n",
    "        },\n",
    "        \"tunable_params\": TUNABLE_PARAMS\n",
    "    },\n",
    "    \"Weighted Subsample RF\": {\n",
    "        \"dataset_template\": \"simple_random_{sample_size}.csv\",\n",
    "        \"model\": RandomForestClassifier,\n",
    "        \"fixed_params\": {\n",
    "            \"class_weight\": \"balanced_subsample\",\n",
    "            \"bootstrap\": True\n",
    "        },\n",
    "        \"tunable_params\": TUNABLE_PARAMS\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/samples\"\n",
    "aoi_sites = [\"aoi1_2024_06_02\", \"aoi2_2024_06_02\", \"aoi3_2024_06_02\", \"aoi4_2024_06_02\"]\n",
    "sample_sizes = [100] #[100, 1000, 10000]  # Sample sizes to evaluate\n",
    "iterations = 1  # Number of times to repeat the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Iteration 1...\n",
      "Iteration 1: Completed Simple Random Sampling, Sample size: 100, Testing site: aoi1_2024_06_02 in 14.13 seconds\n",
      "Iteration 1: Completed SMOTE Balanced Sampling, Sample size: 100, Testing site: aoi1_2024_06_02 in 10.44 seconds\n",
      "Iteration 1: Completed Stratified Balanced Sampling, Sample size: 100, Testing site: aoi1_2024_06_02 in 9.58 seconds\n",
      "Iteration 1: Completed Balanced RF, Sample size: 100, Testing site: aoi1_2024_06_02 in 12.35 seconds\n"
     ]
    }
   ],
   "source": [
    "# Centralized results storage\n",
    "all_results = []  # Store results from all iterations\n",
    "\n",
    "for iteration in range(1, iterations + 1):\n",
    "    print(f\"Starting Iteration {iteration}...\")\n",
    "    iteration_results = []  # Store results for this iteration\n",
    "\n",
    "    # Leave-One-Site-Out Cross-Validation for each sample size\n",
    "    for sample_size in sample_sizes:\n",
    "        for left_out_site in aoi_sites:\n",
    "            train_sites = [site for site in aoi_sites if site != left_out_site]\n",
    "            test_site = left_out_site\n",
    "\n",
    "            for config_name in configurations.keys():\n",
    "                start_time = time.time()  # Start time for the evaluation\n",
    "\n",
    "                # Load training data dynamically\n",
    "                train_data = pd.concat([\n",
    "                    pd.read_csv(get_dataset_path(configurations, config_name, site, sample_size, data_path, is_train=True))\n",
    "                    for site in train_sites\n",
    "                ])\n",
    "                # Load testing data (fixed stratified balanced dataset)\n",
    "                test_data = pd.read_csv(get_dataset_path(configurations, config_name, test_site, sample_size, data_path, is_train=False))\n",
    "                test_data_balanced = pd.read_csv(get_dataset_path(configurations, config_name, test_site, sample_size, data_path, is_train=False, is_balanced=True))\n",
    "\n",
    "                # Prepare features and labels\n",
    "                y_train = train_data['label']\n",
    "                y_test = test_data['label']\n",
    "                y_test_balanced = test_data_balanced['label']\n",
    "\n",
    "                X_train = train_data.drop(columns=['label', 'longitude', 'latitude', 'synthetic'], errors='ignore')\n",
    "                X_test = test_data.drop(columns=['label', 'longitude', 'latitude', 'synthetic'], errors='ignore')\n",
    "                X_test_balanced = test_data_balanced.drop(columns=['label', 'longitude', 'latitude', 'synthetic'], errors='ignore')\n",
    "                \n",
    "                # Initialize and train the untuned model\n",
    "                model = initialize_model(configurations, config_name)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                 \n",
    "                # Evaluate the untuned model\n",
    "                oa, f1 = evaluate_model(model, X_test, y_test)\n",
    "                oa_balanced, f1_balanced = evaluate_model(model, X_test_balanced, y_test_balanced)\n",
    "\n",
    "                # Tune hyperparameters and get the best model\n",
    "                def tune_model_hyperparameters(configurations, X_train, y_train, config_name, iteration, sample_size):\n",
    "                    \"\"\"\n",
    "                    Tune the hyperparameters of the model using Bayesian optimization.\n",
    "                    Returns the best model, parameters, and optimization results.\n",
    "                    \"\"\"\n",
    "                    config = configurations[config_name]\n",
    "                    model = config[\"model\"](**config[\"fixed_params\"])\n",
    "                    \n",
    "                    # Define the search space using tunable_params\n",
    "                    search_space = config[\"tunable_params\"]\n",
    "                    \n",
    "                    # Initialize BayesSearchCV for hyperparameter tuning\n",
    "                    bayes_search = BayesSearchCV(\n",
    "                        estimator=model,\n",
    "                        search_spaces=search_space,\n",
    "                        n_iter=20,\n",
    "                        scoring=\"f1_weighted\",\n",
    "                        cv=3,\n",
    "                        n_jobs=-1,\n",
    "                        random_state=42,\n",
    "                        return_train_score=True\n",
    "                    )\n",
    "                    \n",
    "                    # Perform the search\n",
    "                    bayes_search.fit(X_train, y_train)\n",
    "                    \n",
    "                    # Create directory for optimization plots\n",
    "                    plot_dir = \"../data/results/optimization_objective_plot\"\n",
    "                    os.makedirs(plot_dir, exist_ok=True)\n",
    "                    \n",
    "                    # Create and save the objective plot\n",
    "                    try:\n",
    "                        # Set smaller font sizes for this plot\n",
    "                        plt.rcParams.update({\n",
    "                            'font.size': 6,\n",
    "                            'axes.labelsize': 6,\n",
    "                            'axes.titlesize': 6,\n",
    "                            'xtick.labelsize': 6,\n",
    "                            'ytick.labelsize': 6,\n",
    "                            'legend.fontsize': 6\n",
    "                        })\n",
    "                        \n",
    "                        # Create figure with explicit figure number\n",
    "                        fig = plt.figure(figsize=(12, 8))\n",
    "                        plot_objective(bayes_search.optimizer_results_[0])\n",
    "                        plt.tight_layout()\n",
    "                        \n",
    "                        # Save plot with informative filename\n",
    "                        plot_path = f\"{plot_dir}/objective_iter{iteration}_{config_name}_sample{sample_size}.svg\"\n",
    "                        plt.savefig(plot_path, bbox_inches='tight')\n",
    "                        plt.close(fig)\n",
    "\n",
    "                    finally:\n",
    "                        # Ensure figure is closed even if an error occurs\n",
    "                        plt.close(fig)\n",
    "                        # Reset to default font sizes\n",
    "                        plt.rcParams.update({\n",
    "                            'font.size': 10,\n",
    "                            'axes.labelsize': 10,\n",
    "                            'axes.titlesize': 12,\n",
    "                            'xtick.labelsize': 10,\n",
    "                            'ytick.labelsize': 10,\n",
    "                            'legend.fontsize': 10\n",
    "                        })\n",
    "                    \n",
    "                    return bayes_search.best_estimator_, bayes_search.best_params_\n",
    "\n",
    "                best_model, best_params = tune_model_hyperparameters(\n",
    "                    configurations,\n",
    "                    X_train, \n",
    "                    y_train, \n",
    "                    config_name,\n",
    "                    iteration,\n",
    "                    sample_size\n",
    "                )\n",
    "                # Evaluate the best model \n",
    "                oa_tuned, f1_tuned = evaluate_model(best_model, X_test, y_test)\n",
    "                oa_balanced_tuned, f1_balanced_tuned = evaluate_model(best_model, X_test_balanced, y_test_balanced)\n",
    "            \n",
    "                # Record end time\n",
    "                elapsed_time = time.time() - start_time\n",
    "\n",
    "                # Store results\n",
    "                iteration_results.append({\n",
    "                    \"iteration\": iteration,\n",
    "                    \"configuration\": config_name,\n",
    "                    \"site_left_out\": test_site,\n",
    "                    \"sample_size\": sample_size,\n",
    "                    \"OA\": oa,\n",
    "                    \"balanced_OA\": oa_balanced,\n",
    "                    \"F1\": f1,\n",
    "                    \"balanced_F1\": f1_balanced, \n",
    "                    \"OA_tuned\": oa_tuned,\n",
    "                    \"balanced_OA_tuned\": oa_balanced_tuned,\n",
    "                    \"F1_tuned\": f1,\n",
    "                    \"balanced_F1_tuned\": f1_balanced_tuned,\n",
    "                    \"time_seconds\": elapsed_time,\n",
    "                    **{\"param_\" + key: value for key, value in best_params.items()}\n",
    "                })\n",
    "\n",
    "                # Print completion message with time taken\n",
    "                print(f\"Iteration {iteration}: Completed {config_name}, Sample size: {sample_size}, Testing site: {test_site} in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    # Append results of this iteration to the overall results\n",
    "    all_results.extend(iteration_results)\n",
    "    \n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save results to CSV\n",
    "results_csv_path = \"../data/results/evaluation_results_tuned.csv\"\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "\n",
    "# Display summary of results\n",
    "print(f\"Completed {iterations} iterations. Final Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'generate_optimal_parameter_table' from 'hyperparameter_tuning' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhyperparameter_tuning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate_optimal_parameter_table\n\u001b[32m      3\u001b[39m generate_optimal_parameter_table(\n\u001b[32m      4\u001b[39m     input_path=\u001b[33m\"\u001b[39m\u001b[33m../data/results/evaluation_results_tuned.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     output_path=\u001b[33m\"\u001b[39m\u001b[33m../data/results/optimal_parameters\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'generate_optimal_parameter_table' from 'hyperparameter_tuning' (unknown location)"
     ]
    }
   ],
   "source": [
    "from hyperparameter_tuning import generate_optimal_parameter_table\n",
    "\n",
    "generate_optimal_parameter_table(\n",
    "    input_path=\"../data/results/evaluation_results_tuned.csv\",\n",
    "    output_path=\"../data/results/optimal_parameters\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'configurations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotting_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plot_metric_comparison\n\u001b[32m      3\u001b[39m plot_metric_comparison(\n\u001b[32m      4\u001b[39m     results_df=results_df,  \u001b[38;5;66;03m# Pass the DataFrame directly\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     configurations=\u001b[43mconfigurations\u001b[49m,\n\u001b[32m      6\u001b[39m     metric_pair=(\u001b[33m\"\u001b[39m\u001b[33mOA\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mOA_tuned\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      7\u001b[39m     sample_size=\u001b[32m100\u001b[39m,\n\u001b[32m      8\u001b[39m     y_limit = (\u001b[32m0.5\u001b[39m, \u001b[32m1.0\u001b[39m),\n\u001b[32m      9\u001b[39m     output_path=\u001b[33m\"\u001b[39m\u001b[33m../data/plots/evaluation_results_tuned.svg\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Changed to .svg\u001b[39;00m\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'configurations' is not defined"
     ]
    }
   ],
   "source": [
    "from plotting_functions import plot_metric_comparison\n",
    "\n",
    "plot_metric_comparison(\n",
    "    results_df=results_df,  # Pass the DataFrame directly\n",
    "    configurations=configurations,\n",
    "    metric_pair=(\"OA\", \"OA_tuned\"),\n",
    "    sample_size=100,\n",
    "    y_limit = (0.5, 1.0),\n",
    "    output_path=\"../data/plots/evaluation_results_tuned.svg\"  # Changed to .svg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'configurations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotting_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plot_metric_comparison\n\u001b[32m      3\u001b[39m plot_metric_comparison(\n\u001b[32m      4\u001b[39m     results_df=results_df,  \u001b[38;5;66;03m# Pass the DataFrame directly\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     configurations=\u001b[43mconfigurations\u001b[49m,\n\u001b[32m      6\u001b[39m     metric_pair=(\u001b[33m\"\u001b[39m\u001b[33mF1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mF1_tuned\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      7\u001b[39m     sample_size=\u001b[32m100\u001b[39m,\n\u001b[32m      8\u001b[39m     output_path=\u001b[33m\"\u001b[39m\u001b[33m../data/plots/evaluation_results_tuned.svg\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Changed to .svg\u001b[39;00m\n\u001b[32m      9\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'configurations' is not defined"
     ]
    }
   ],
   "source": [
    "from plotting_functions import plot_metric_comparison\n",
    "\n",
    "plot_metric_comparison(\n",
    "    results_df=results_df,  # Pass the DataFrame directly\n",
    "    configurations=configurations,\n",
    "    metric_pair=(\"F1\", \"F1_tuned\"),\n",
    "    sample_size=100,\n",
    "    output_path=\"../data/plots/evaluation_results_tuned.svg\"  # Changed to .svg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set_theme(style=\"whitegrid\", font=\"Arial\", font_scale=1)\n",
    "sns.set_context(\"talk\",  font_scale=1, rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "def plot_f1_boxplot(csv_filepath, configurations):\n",
    "    \"\"\"\n",
    "    Plots F1 scores across configurations using Seaborn Catplot with boxplots.\n",
    "    Custom binary colors are used for each sample size, with some set to invisible.\n",
    "    \"\"\"\n",
    "    # Load results from the CSV file\n",
    "    results_df = pd.read_csv(csv_filepath)\n",
    "\n",
    "    # Filter to only include the F1 metric\n",
    "    filtered_results = results_df[[\"configuration\", \"sample_size\", \"F1\"]]\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Define custom color palette for sample sizes\n",
    "    custom_palette = {\n",
    "        100: \"#FFFFFF00\",  # Transparent\n",
    "        1000: \"#9f9f9f\",   # Gray\n",
    "        10000: \"#FFFFFF00\" # Transparent\n",
    "    }\n",
    "\n",
    "    # Create the Seaborn Catplot\n",
    "    catplot = sns.catplot(\n",
    "        data=filtered_results,\n",
    "        x=\"configuration\",\n",
    "        y=\"F1\",\n",
    "        hue=\"sample_size\",\n",
    "        kind=\"box\",\n",
    "        palette=sns.color_palette(\"colorblind\", 3),\n",
    "        height=6,\n",
    "        aspect=3,\n",
    "        legend=False,\n",
    "        linewidth=1.5\n",
    "    )\n",
    "\n",
    "    # Adjust y-axis limits\n",
    "    catplot.set(ylim=(0.85, 1))\n",
    "\n",
    "    # Customize the plot\n",
    "    catplot.set_axis_labels(\"\", \"F1-Score\")\n",
    "    catplot.set_titles(\"F1 Scores Across Configurations and Sample Sizes\")\n",
    "    catplot.set_xticklabels(rotation=45, horizontalalignment=\"right\")\n",
    "\n",
    "    #sns.despine(offset=10, trim=True);\n",
    "\n",
    "    # Save the figure as an SVG\n",
    "    output_filepath = \"results/figures/configurations/f1_boxplot.svg\"\n",
    "    plt.savefig(output_filepath, format=\"svg\", bbox_inches=\"tight\")\n",
    "    print(f\"Plot saved to {output_filepath}\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example call to the function\n",
    "csv_filepath = \"results/data/evaluation_results.csv\"  # Path to the CSV file\n",
    "plot_f1_boxplot(csv_filepath, configurations)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_class_imbalance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
